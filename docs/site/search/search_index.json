{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"RADTorch"},{"location":"about/","text":"About RADTorch RADTorch provides a framework of higher level classes and functions that significantly reduce the time needed for implementation of different machine and deep learning algorithms on DICOM and non-DICOM medical images. RADTorch was built by radiologists for radiologists so they can build, test and implement state-of-the-art machine learning algorithms in minutes. RADTorch is : User-Friendly: Minimal coding required. Comprehensive: Includes data analysis and visualization. Backed by state-of-the-art machine learning and data visualization frameworks. Meet the Team Mohamed Elbanan, MD Founder Radiologist-in-training and Artificial Intelligence Advocate.","title":"About"},{"location":"about/#about-radtorch","text":"RADTorch provides a framework of higher level classes and functions that significantly reduce the time needed for implementation of different machine and deep learning algorithms on DICOM and non-DICOM medical images. RADTorch was built by radiologists for radiologists so they can build, test and implement state-of-the-art machine learning algorithms in minutes. RADTorch is : User-Friendly: Minimal coding required. Comprehensive: Includes data analysis and visualization. Backed by state-of-the-art machine learning and data visualization frameworks.","title":"About RADTorch"},{"location":"about/#meet-the-team","text":"Mohamed Elbanan, MD Founder Radiologist-in-training and Artificial Intelligence Advocate.","title":"Meet the Team"},{"location":"classifier/","text":"Classifier radtorch.classifier ImageClassifier Class for image classifier. This class acts as wrapper to train a selected model (either pytorch neural network or a sklearn classifier) using a dataset which can be either a radtorch ImageDataset or VolumeDataset . Optionally, a specific train and validation pytorch dataloaders may be manually specified instead of using radtorch dataset objects. Training a Pytroch Neural Network If the model to train is a pytorch neural network, in addition to the model, ImageClassifier expects a pytorch criterion/loss function , a pytorch optimizer and an optional pytorch scheduler . Training an sklearn classifier If the model to be trained is an sklearn classifier, ImageClassifier performs feature extraction followed by training the model. Accordingly, ImageClassifier expects a model architecture for the feature extraction process. Creating multiple classifier objects using same model/neural network object To ensure results consistency, a new instance of pytorch model/neural network object MUST be instatiated with every classifier object. For example, Do this: M = radtorch . model . Model ( model_arch = 'vgg16' , in_channels = 1 , out_classes = 2 ) clf = radtorch . classifier . ImageClassifier ( model = M , dataset = ds ) clf . fit ( epochs = 3 ) M = radtorch . model . Model ( model_arch = 'vgg16' , in_channels = 1 , out_classes = 2 ) clf2 = radtorch . classifier . ImageClassifier ( model = M , dataset = ds ) clf2 . fit ( epochs = 3 ) and Do NOT do this : M = radtorch . model . Model ( model_arch = 'vgg16' , in_channels = 1 , out_classes = 2 ) clf = radtorch . classifier . ImageClassifier ( model = M , dataset = ds ) clf . fit ( epochs = 3 ) clf2 = radtorch . classifier . ImageClassifier ( model = M , dataset = ds ) clf2 . fit ( epochs = 3 ) Parameters: Name Type Description Default name str Name to be give to the Image Classifier. If none provided, the current date and time will be used to created a generic classifier name. None model pytroch neural network or sklearn classifier Model to be trained. None dataset ImageDataset or VolumeDataset ImageDataset or VolumeDataset to be used for training. None dataloader_train pytorch dataloader Optional Training pytorch DataLoader None dataloader_valid pytorch dataloader Optional Validation pytorch DataLoader None device str Device to be used for training. 'auto' feature_extractor_arch str Architecture of the model to be used for feature extraction when training sklearn classifier. See (https://pytorch.org/vision/0.8/models.html#classification)[https://pytorch.org/vision/0.8/models.html#classification] 'vgg16' criterion pytorch loss function Loss function to be used during training a pytorch neural network. None optimizer pytorch optimizer Loss function to be used during training a pytorch neural network. None scheduler pytorch scheduler Scheduler to be used during training a pytorch neural network. None scheduler metric (str when using ReduceLROnPlateau pytorch scheduler, a target loss or accuracy must be provided to monitor. Options: 'train_loss', 'train_accuracy', 'valid_loss', 'valid_accuracy'. None use_checkpoint bool Path (str) to a saved checkpoint to continue training. If a checkpoint is used to resume training, training will be resumed from saved checkpoint to new/specified epoch number. False random_seed int Random seed (default=100) 0 Using manual pytorch dataloaders If maually created dataloaders are used, set dataset to None. Selecting device for training Auto mode automatically detects if there is GPU utilizes it for training. Attributes: Name Type Description type str Type of the classifier model to be trained. train_losses list List of train losses recorded. Length = Number of epochs. valid_losses list List of validation losses recorded. Length = Number of epochs. train_acc list List of train accuracies recorded. Length = Number of epochs. valid_acc list List of validation accuracies recorded. Length = Number of epochs. valid_loss_min float Minimum Validation Loss to save checkpoint. best_model pytroch neural network or sklearn classifier Best trained model with lowest Validation Loss in case of pytorch neural networks or the trained classifier for sklearn classifiers. train_logs pandas dataframe Table/Dataframe with all train/validation losses. Methods fit ( self , ** kwargs ) Trains the ImageClassifier object. Training a Model All the following arguments, except auto_save_ckpt and random_seed , apply only when training a pytorch neural network model. Training sklearn classifier does not need arguments. Parameters: Name Type Description Default epochs int Number of training epochs (default: 20). required valid bool True to perform validation after each train step. False to only train on training dataset without validation. (default: True) required print_every int Number of epochs after which print results. (default: 1) required target_valid_loss float / string Minimum value to automatically save trained model afterwards. If 'lowest' is used, with every epoch , if the validation loss is less than minimum, then new best model is saved in checkpoint. Accepts maunally specified float minimum loss. (default: 'lowest') required auto_save_ckpt bool Automatically save chekpoints. If True, a checkpoint file is saved. Please see below. (default: False) required random_seed int Random seed. (default: 100) required verbose int Verbose level during training. Options: 0, 1, 2. (default: 2) required Using auto_save_ckpt If auto_save_ckpt is True, whenever training target is achieved, a new checkpoint will be saved. The checkpoint file name = ImageClassifier.name+'epoch'+str(current_epoch)+'.checkpoint' e.g. If the checkpoint is saved at epoch 10 for an ImageClassifier named clf, the checkpoint file will be named: clf_epoch_10.chekpoint Resuming training using a saved checkpoint file When using a saved checkpoint to resume training, a new instance of the Model /Pytorch Model and ImageClassifier should be instantiated. For example: # Intial Training M = radtorch . model . Model ( model_arch = 'vgg16' , in_channels = 1 , out_classes = 2 ) clf = radtorch . classifier . ImageClassifier ( M , dataset ) clf . fit ( auto_save_ckpt = True , epochs = 5 , verbose = 3 ) # Saves the best checkpoint automatically # Resume Training M = radtorch . model . Model ( model_arch = 'vgg16' , in_channels = 1 , out_classes = 2 ) clf2 = radtorch . classifier . ImageClassifier ( M , dataset , use_checkpoint = 'saved_ckpt.checkpoint' ) clf2 . fit ( auto_save_ckpt = False , epochs = 5 , verbose = 3 ) Checkpoint Files A checkpoint file is a dictionary of: timestamp : Timestamp when saving the checkpoint. type : ImageClassifier type. classifier : ImageClassifier object. epochs : Total epochs specified on initial training. current_epoch : Current epoch when checkpoint was saved. optimizer_state_dict : Current state of Optimizer. train_losses : List of train losses recorded valid_losses : List of validation losses recorded valid_loss_min : Min Validation loss - See above. info ( self ) Displays all information about the ImageClassifier object.","title":"Classifier <small>radtorch.classifier</small>"},{"location":"classifier/#classifier-radtorchclassifier","text":"","title":"Classifier radtorch.classifier"},{"location":"classifier/#radtorch.classifier.ImageClassifier","text":"Class for image classifier. This class acts as wrapper to train a selected model (either pytorch neural network or a sklearn classifier) using a dataset which can be either a radtorch ImageDataset or VolumeDataset . Optionally, a specific train and validation pytorch dataloaders may be manually specified instead of using radtorch dataset objects. Training a Pytroch Neural Network If the model to train is a pytorch neural network, in addition to the model, ImageClassifier expects a pytorch criterion/loss function , a pytorch optimizer and an optional pytorch scheduler . Training an sklearn classifier If the model to be trained is an sklearn classifier, ImageClassifier performs feature extraction followed by training the model. Accordingly, ImageClassifier expects a model architecture for the feature extraction process. Creating multiple classifier objects using same model/neural network object To ensure results consistency, a new instance of pytorch model/neural network object MUST be instatiated with every classifier object. For example, Do this: M = radtorch . model . Model ( model_arch = 'vgg16' , in_channels = 1 , out_classes = 2 ) clf = radtorch . classifier . ImageClassifier ( model = M , dataset = ds ) clf . fit ( epochs = 3 ) M = radtorch . model . Model ( model_arch = 'vgg16' , in_channels = 1 , out_classes = 2 ) clf2 = radtorch . classifier . ImageClassifier ( model = M , dataset = ds ) clf2 . fit ( epochs = 3 ) and Do NOT do this : M = radtorch . model . Model ( model_arch = 'vgg16' , in_channels = 1 , out_classes = 2 ) clf = radtorch . classifier . ImageClassifier ( model = M , dataset = ds ) clf . fit ( epochs = 3 ) clf2 = radtorch . classifier . ImageClassifier ( model = M , dataset = ds ) clf2 . fit ( epochs = 3 ) Parameters: Name Type Description Default name str Name to be give to the Image Classifier. If none provided, the current date and time will be used to created a generic classifier name. None model pytroch neural network or sklearn classifier Model to be trained. None dataset ImageDataset or VolumeDataset ImageDataset or VolumeDataset to be used for training. None dataloader_train pytorch dataloader Optional Training pytorch DataLoader None dataloader_valid pytorch dataloader Optional Validation pytorch DataLoader None device str Device to be used for training. 'auto' feature_extractor_arch str Architecture of the model to be used for feature extraction when training sklearn classifier. See (https://pytorch.org/vision/0.8/models.html#classification)[https://pytorch.org/vision/0.8/models.html#classification] 'vgg16' criterion pytorch loss function Loss function to be used during training a pytorch neural network. None optimizer pytorch optimizer Loss function to be used during training a pytorch neural network. None scheduler pytorch scheduler Scheduler to be used during training a pytorch neural network. None scheduler metric (str when using ReduceLROnPlateau pytorch scheduler, a target loss or accuracy must be provided to monitor. Options: 'train_loss', 'train_accuracy', 'valid_loss', 'valid_accuracy'. None use_checkpoint bool Path (str) to a saved checkpoint to continue training. If a checkpoint is used to resume training, training will be resumed from saved checkpoint to new/specified epoch number. False random_seed int Random seed (default=100) 0 Using manual pytorch dataloaders If maually created dataloaders are used, set dataset to None. Selecting device for training Auto mode automatically detects if there is GPU utilizes it for training. Attributes: Name Type Description type str Type of the classifier model to be trained. train_losses list List of train losses recorded. Length = Number of epochs. valid_losses list List of validation losses recorded. Length = Number of epochs. train_acc list List of train accuracies recorded. Length = Number of epochs. valid_acc list List of validation accuracies recorded. Length = Number of epochs. valid_loss_min float Minimum Validation Loss to save checkpoint. best_model pytroch neural network or sklearn classifier Best trained model with lowest Validation Loss in case of pytorch neural networks or the trained classifier for sklearn classifiers. train_logs pandas dataframe Table/Dataframe with all train/validation losses.","title":"ImageClassifier"},{"location":"classifier/#radtorch.classifier.ImageClassifier-methods","text":"","title":"Methods"},{"location":"classifier/#radtorch.classifier.ImageClassifier.fit","text":"Trains the ImageClassifier object. Training a Model All the following arguments, except auto_save_ckpt and random_seed , apply only when training a pytorch neural network model. Training sklearn classifier does not need arguments. Parameters: Name Type Description Default epochs int Number of training epochs (default: 20). required valid bool True to perform validation after each train step. False to only train on training dataset without validation. (default: True) required print_every int Number of epochs after which print results. (default: 1) required target_valid_loss float / string Minimum value to automatically save trained model afterwards. If 'lowest' is used, with every epoch , if the validation loss is less than minimum, then new best model is saved in checkpoint. Accepts maunally specified float minimum loss. (default: 'lowest') required auto_save_ckpt bool Automatically save chekpoints. If True, a checkpoint file is saved. Please see below. (default: False) required random_seed int Random seed. (default: 100) required verbose int Verbose level during training. Options: 0, 1, 2. (default: 2) required Using auto_save_ckpt If auto_save_ckpt is True, whenever training target is achieved, a new checkpoint will be saved. The checkpoint file name = ImageClassifier.name+'epoch'+str(current_epoch)+'.checkpoint' e.g. If the checkpoint is saved at epoch 10 for an ImageClassifier named clf, the checkpoint file will be named: clf_epoch_10.chekpoint Resuming training using a saved checkpoint file When using a saved checkpoint to resume training, a new instance of the Model /Pytorch Model and ImageClassifier should be instantiated. For example: # Intial Training M = radtorch . model . Model ( model_arch = 'vgg16' , in_channels = 1 , out_classes = 2 ) clf = radtorch . classifier . ImageClassifier ( M , dataset ) clf . fit ( auto_save_ckpt = True , epochs = 5 , verbose = 3 ) # Saves the best checkpoint automatically # Resume Training M = radtorch . model . Model ( model_arch = 'vgg16' , in_channels = 1 , out_classes = 2 ) clf2 = radtorch . classifier . ImageClassifier ( M , dataset , use_checkpoint = 'saved_ckpt.checkpoint' ) clf2 . fit ( auto_save_ckpt = False , epochs = 5 , verbose = 3 ) Checkpoint Files A checkpoint file is a dictionary of: timestamp : Timestamp when saving the checkpoint. type : ImageClassifier type. classifier : ImageClassifier object. epochs : Total epochs specified on initial training. current_epoch : Current epoch when checkpoint was saved. optimizer_state_dict : Current state of Optimizer. train_losses : List of train losses recorded valid_losses : List of validation losses recorded valid_loss_min : Min Validation loss - See above.","title":"fit()"},{"location":"classifier/#radtorch.classifier.ImageClassifier.info","text":"Displays all information about the ImageClassifier object.","title":"info()"},{"location":"copyrights/","text":"Copyrights GNU Affero General Public License v3.0 License Copyright (c) 2020-2022 RADTorch, Mohamed Elbanan M.D. This program is free software: you can redistribute it and/or modify it under the terms of the GNU Affero General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Affero General Public License for more details. You should have received a copy of the GNU Affero General Public License along with this program. If not, see https://www.gnu.org/licenses/.","title":"Copyrights"},{"location":"copyrights/#copyrights","text":"GNU Affero General Public License v3.0 License Copyright (c) 2020-2022 RADTorch, Mohamed Elbanan M.D. This program is free software: you can redistribute it and/or modify it under the terms of the GNU Affero General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Affero General Public License for more details. You should have received a copy of the GNU Affero General Public License along with this program. If not, see https://www.gnu.org/licenses/.","title":"Copyrights"},{"location":"data/","text":"Data radtorch.data One of the core functions of radtorch is the ability to handle different types of medical/non-medical DICOM/non-DICOM images efficiently with ease. Below is list of classes that make the magic happen. ImageObject Creates a 3D tensor whose dimensions = [channels, width, height] from an image path. Parameters: Name Type Description Default path str Path to an image. required out_channels int Number of output channels. Only 1 and 3 channels supported. required transforms list Albumentations transformations. See Image Augmentation . required WW int or list Window width for DICOM images. Single value if using 1 channel or list of 3 values for 3 channels. See https://radiopaedia.org/articles/windowing-ct . required WL int or list Window level for DICOM images. Single value if using 1 channel or list of 3 values for 3 channels. See https://radiopaedia.org/articles/windowing-ct . required Returns: Type Description tensor 3D tensor whose dimensions = [channels, width, height] Examples: >>> i = radtorch . data . ImageObject ( path = 'data/PROTOTYPE/DIRECTORY/abdomen/abd_1/1-001.dcm' ) >>> i . shape torch . Size ([ 1 , 512 , 512 ]) VolumeObject Creates an Image Volume Object (4D tensor) from series images contained in a folder. Parameters: Name Type Description Default directory str Folder containing series/sequence images. Images must be DICOM files. required out_channels int Number of output channels. Only 1 and 3 channels supported. required transforms list Albumentations transformations. See https://albumentations.ai/docs/getting_started/image_augmentation/ . required WW int or list Window width for DICOM images. Single value if using 1 channel or list of 3 values for 3 channels. See https://radiopaedia.org/articles/windowing-ct . required WL int or list Window level for DICOM images. Single value if using 1 channel or list of 3 values for 3 channels. See https://radiopaedia.org/articles/windowing-ct . required Returns: Type Description tensor 4D tensor with dimensions = [channels, number_images/depth, width, height]. See https://pytorch.org/docs/stable/generated/torch.nn.Conv3d.html Examples: >>> i = radtorch . data . VolumeObject ( directory = 'data/PROTOTYPE/DIRECTORY/abdomen/abd_1' ) >>> i . shape torch . Size ([ 1 , 40 , 512 , 512 ]) ImageDataset ( Dataset ) Creates pytorch dataset(s) and dataloader(s) objects from a parent folder. Use this class for image tasks that invovles handling each single image as a single instance of your dataset. Examples: import radtorch import albumentations as A # Specify image transformations T = A . Compose ([ A . Resize ( 256 , 256 )]) # Create dataset object ds = radtorch . data . ImageDataset ( folder = 'data/4CLASS/' , split = { 'valid' : 0.2 , 'test' : 0.2 }, out_channels = 1 , transforms = { 'train' : T , 'valid' : T , 'test' : T }, ) ds . data_stat () ds . table Parameters: Name Type Description Default folder str Parent folder containing images. radtorch.ImageDataset expects images to be arranged in the following structure: root/ class_1/ image_1 image_2 ... class_2/ image_1 image_2 ... required name str Name to be give to the dataset. If none provided, the current date and time will be used to created a generic dataset name. (default=None) None label_table str|dataframe The table containing data labels for your images. Expected table should contain at least 2 columns: image path column and a label column. Table can be string path to CSV or a pandas dataframe object.(default=None) None instance_id bool True if the data provided in the image path column in label_table contains the image id not the absolute path for the image. (default= False) False add_extension bool If instance_id =True, use this to add extension to image path as needed. Extension must be provided without \".\" e.g. \"dcm\". (default=False) False out_channels int Number of output channels. (default=1) 1 WW int or list Window width for DICOM images. Single value if using 1 channel or list of 3 values for 3 channels. See https://radiopaedia.org/articles/windowing-ct . None WL int or list Window level for DICOM images. Single value if using 1 channel or list of 3 values for 3 channels. See https://radiopaedia.org/articles/windowing-ct . None path_col str Name of the column containing image path data in the label_table. (default='path') 'path' label_col str Name of the column containing label data in the label_table. (default='label') 'label' extension str Type/Extension of images. (default='dcm') 'dcm' transforms dict Dictionary of Albumentations transformations in the form of {'train': .. , 'valid': .. , 'test': .. }. See https://albumentations.ai/docs/getting_started/image_augmentation/ . (default=None) None random_state int Random seed (default=100) 100 sample float Sample or percent of the overall data to be used. (default=1.0) 1.0 split dict dictionary defining how data will be split for training/validation/testing. Follows the sturcture {'valid': float, 'test': float} or {'valid':'float'} in case no testing subset is needed. The percent of the training subset is infered automatically. False ignore_zero_img bool True to ignore images containig all zero pixels. (default=False) False normalize bool True to normalize image data between 0 and 1. (default=True) True batch_size int Dataloader batch size. (default = 16) 16 shuffle bool True to shuffle images during training. (default=True) True weighted_sampler bool True to use a weighted sampler for unbalanced datasets. See https://pytorch.org/docs/stable/data.html#torch.utils.data.WeightedRandomSampler. (default=False) False num_workers int Dataloader CPU workers. (default = 0) 0 Attributes: Name Type Description classes list List of generated classes/labels. class_to_idx dict Dictionary of generated classes/labels and corresponding class/label id. idx_train list List of index values of images/instances used for training subset. These refer to index of ImageDataset.table . idx_valid list List of index values of images/instances used for validation subset. These refer to index of ImageDataset.table . idx_test list List of index values of images/instances used for testing subset. These refer to index of ImageDataset.table . table pandas dataframe Table of images , paths and their labels. table_train pandas dataframe Table of images used for training. Subset of ImageDataset.table . table_valid pandas dataframe Table of images used for validation. Subset of ImageDataset.table . table_test pandas dataframe Table of images used for testing. Subset of ImageDataset.table . tables dict Dictionary of all generated tables in the form of: {'train': table, 'valid':table, 'test':table}. dataset_train pytorch dataset object Training pytorch Dataset dataset_valid pytorch dataset object Validation pytorch Dataset dataset_test pytorch dataset object Testing pytorch Dataset datasets dict Dictionary of all generated Datasets in the form of: {'train': Dataset, 'valid':Dataset, 'test':Dataset}. dataloader_train pytorch dataloader object Training pytorch DataLoader dataloader_valid pytorch dataloader object Validation pytorch DataLoader dataloader_test pytorch dataloader object Testing pytorch DataLoader dataloaders dict Dictionary of all generated Dataloaders in the form of: {'train': Dataloader, 'valid':Dataloader, 'test':Dataloader}. class_weights tensor Values of class weights, for imbalanced datasets, to be used to weight loss functions. See https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss . sampler_weights tensor Values of vlass weights, for imbalanced datasets, to be used to sample from the dataset using Pytroch WeightedRandomSampler. Affects only training dataset not validation or testing. See https://pytorch.org/docs/stable/data.html#torch.utils.data.WeightedRandomSampler Methods data_stat ( self , plot = True , figsize = ( 8 , 6 ), cmap = 'viridis' ) Displays distribution of classes across subsets as table or plot. Parameters: Name Type Description Default plot bool True, display data as figure. False, display data as table. True figsize tuple size of the displayed figure. (8, 6) cmap string Name of Matplotlib color map to be used. See Matplotlib colormaps 'viridis' Returns: Type Description pandas dataframe if plot=False Examples: ds = radtorch . data . ImageDataset ( folder = 'data/4CLASS/' , out_channels = 1 ) ds . data_stat () view_batch ( self , subset = 'train' , figsize = ( 15 , 5 ), cmap = 'gray' , num_images = False , rows = 2 ) Displays a batch from a certain subset. Parameters: Name Type Description Default subset string Datasubset to use: either 'train', 'valid', or 'test'. 'train' figsize tuple Size of the displayed figure. (15, 5) cmap string Name of Matplotlib color map to be used. See Matplotlib colormaps 'gray' num_images int Number of displayed image. Usually equals batch_size unless otherwise specified. False rows int Number of rows. 2 Returns: Type Description figure figure containing samples Examples: ds = radtorch . data . ImageDataset ( folder = 'data/4CLASS/' , out_channels = 1 ) ds . view_batch () view_image ( self , id = 0 , figsize = ( 25 , 5 ), cmap = 'gray' ) Displays separate images/channels of an image. Parameters: Name Type Description Default id int Target image id in dataset.table (row index). 0 figsize tuple size of the displayed figure. (25, 5) cmap string Name of Matplotlib color map to be used. See Matplotlib colormaps 'gray' Returns: Type Description figure figure containing samples Examples: ds = radtorch . data . ImageDataset ( folder = 'data/4CLASS/' , out_channels = 3 ) ds . view_image ( id = 9 ) ds = radtorch . data . ImageDataset ( folder = 'data/4CLASS/' , out_channels = 3 , WW = [ 1500 , 350 , 80 ], WL = [ - 600 , 50 , 40 ]) ds . view_image ( id = 9 ) VolumeDataset ( Dataset ) Dataset object for DICOM Volume. Creates dataset(s) and dataloader(s) ready for training using radtorch or pytorch directly. Parameters: Name Type Description Default folder str Parent folder containing images. radtorch.VolumeDataset expects files to be arranged in the following structure: root/ class_1/ sequence_1/ image_1 image_2 ... sequence_2/ image_1 image_2 ... class_2/ sequence_1/ image_1 image_2 ... sequence_2/ image_1 image_2 ... ... required name str Name to be give to the dataset. If none provided, the current date and time will be used to created a generic dataset name. (default=None) None label_table str|dataframe The table containing data labels for your images. Expected table should contain at least 2 columns: image path column and a label column. Table can be string path to CSV or a pandas dataframe object.(default=None) None use_file bool True to use pre-generated/resampled volume files. To use Volume files: Volume files should be created using radtorch.data.VolumeObject Saved with extension .pt . Placed in the sequence folder. False extension str Type/Extension of images. 'dcm' out_channels int Number of output channels. 1 WW int or list Window width for DICOM images. Single value if using 1 channel or list of 3 values for 3 channels. See https://radiopaedia.org/articles/windowing-ct . None WL int or list Window level for DICOM images. Single value if using 1 channel or list of 3 values for 3 channels. See https://radiopaedia.org/articles/windowing-ct . None path_col str Name of the column containing image path data in the label_table. 'path' label_col str Name of the column containing label data in the label_table. 'label' study_col str Name of the column containing study id in label_table. 'study_id' transforms dict Dictionary of Albumentations transformations in the form of {'train': .. , 'valid': .. , 'test': .. }. See https://albumentations.ai/docs/getting_started/image_augmentation/. NOTE: If using already resampled/created volume files, transformation should be applied during volume creation not dataset i.e. Transforms specified here have no effect during training. None random_state int Random seed. 100 sample float Sample or percent of the overall data to be used. 1.0 split dict dictionary defining how data will be split for training/validation/testing. Follows the sturcture {'valid': float, 'test': float} or {'valid':'float'} in case no testing subset is needed. The percent of the training subset is infered automatically. False normalize bool True to normalize image data between 0 and 1. True batch_size int Dataloader batch size. 16 shuffle bool True to shuffle images during training. True weighted_sampler bool True to use a weighted sampler for unbalanced datasets. See https://pytorch.org/docs/stable/data.html#torch.utils.data.WeightedRandomSampler. False num_workers int Dataloader CPU workers. 0 Attributes: Name Type Description classes list List of generated classes/labels. class_to_idx dict Dictionary of generated classes/labels and corresponding class/label id. idx_train list List of index values of images/instances used for training subset. These refer to index of ImageDataset.table . idx_valid list List of index values of images/instances used for validation subset. These refer to index of ImageDataset.table . idx_test list List of index values of images/instances used for testing subset. These refer to index of ImageDataset.table . table pandas dataframe Table of images , paths and their labels. table_train pandas dataframe Table of images used for training. Subset of ImageDataset.table . table_valid pandas dataframe Table of images used for validation. Subset of ImageDataset.table . table_test pandas dataframe Table of images used for testing. Subset of ImageDataset.table . tables dict Dictionary of all generated tables in the form of: {'train': table, 'valid':table, 'test':table}. dataset_train pytorch dataset object Training pytorch Dataset dataset_valid pytorch dataset object Validation pytorch Dataset dataset_test pytorch dataset object Testing pytorch Dataset datasets dict Dictionary of all generated Datasets in the form of: {'train': Dataset, 'valid':Dataset, 'test':Dataset}. dataloader_train pytorch dataloader object Training pytorch DataLoader dataloader_valid pytorch dataloader object Validation pytorch DataLoader dataloader_test pytorch dataloader object Testing pytorch DataLoader dataloaders dict Dictionary of all generated Dataloaders in the form of: {'train': Dataloader, 'valid':Dataloader, 'test':Dataloader}. class_weights tensor Values of class weights, for imbalanced datasets, to be used to weight loss functions. See https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss . sampler_weights tensor Values of vlass weights, for imbalanced datasets, to be used to sample from the dataset using Pytroch WeightedRandomSampler. Affects only training dataset not validation or testing. See https://pytorch.org/docs/stable/data.html#torch.utils.data.WeightedRandomSampler Examples: import radtorch import albumentations as A # Specify image transformations T = A . Compose ([ A . Resize ( 256 , 256 )]) # Create dataset object ds = radtorch . data . VolumeDataset ( folder = 'data/PROTOTYPE/DIRECTORY/' , split = { 'valid' : 0.3 , 'test' : 0.3 }, out_channels = 1 , transforms = { 'train' : T , 'valid' : T , 'test' : T }, ) ds . data_stat () ds . table Methods data_stat ( self , plot = True , figsize = ( 8 , 6 ), cmap = 'viridis' ) Displays distribution of classes across subsets as table or plot. Parameters: Name Type Description Default plot bool True, display data as figure. False, display data as table. True figsize tuple size of the displayed figure. (8, 6) cmap string Name of Matplotlib color map to be used. See Matplotlib colormaps 'viridis' Returns: Type Description pandas dataframe if plot=False view_study ( self , id , plane = 'axial' , figsize = ( 15 , 15 ), cols = 5 , rows = 5 , start = 0 , end =- 1 ) Show sample images from a study. Warning This works only with single channel images. Multiple channels are not supported yet here. Parameters: Name Type Description Default id int Target study id in dataset.table (row index). required plane str Anatomical plane to display the images in. Options: 'axial', 'coronal' or 'sagittal'. 'axial' figsize tuple Size of the displayed figure. (15, 15) cols int Number of columns. 5 rows int Number of rows. 5 start int id of the first image to display. 0 end int id of the last image to display. -1 Returns: Type Description figure figure containing images from study. Examples: import radtorch import albumentations as A # Specify image transformations T = A . Compose ([ A . Resize ( 256 , 256 )]) # Create dataset object ds = radtorch . data . VolumeDataset ( folder = 'data/PROTOTYPE/DIRECTORY/' , split = { 'valid' : 0.3 , 'test' : 0.3 }, out_channels = 1 , transforms = { 'train' : T , 'valid' : T , 'test' : T }, ) ds . view_study ( id = 0 , plane = 'axial' ) ds . view_study ( id = 0 , plane = 'coronal' , start = 150 )","title":"Data <small>radtorch.data</small>"},{"location":"data/#data-radtorchdata","text":"One of the core functions of radtorch is the ability to handle different types of medical/non-medical DICOM/non-DICOM images efficiently with ease. Below is list of classes that make the magic happen.","title":"Data radtorch.data"},{"location":"data/#radtorch.data.ImageObject","text":"Creates a 3D tensor whose dimensions = [channels, width, height] from an image path. Parameters: Name Type Description Default path str Path to an image. required out_channels int Number of output channels. Only 1 and 3 channels supported. required transforms list Albumentations transformations. See Image Augmentation . required WW int or list Window width for DICOM images. Single value if using 1 channel or list of 3 values for 3 channels. See https://radiopaedia.org/articles/windowing-ct . required WL int or list Window level for DICOM images. Single value if using 1 channel or list of 3 values for 3 channels. See https://radiopaedia.org/articles/windowing-ct . required Returns: Type Description tensor 3D tensor whose dimensions = [channels, width, height] Examples: >>> i = radtorch . data . ImageObject ( path = 'data/PROTOTYPE/DIRECTORY/abdomen/abd_1/1-001.dcm' ) >>> i . shape torch . Size ([ 1 , 512 , 512 ])","title":"ImageObject"},{"location":"data/#radtorch.data.VolumeObject","text":"Creates an Image Volume Object (4D tensor) from series images contained in a folder. Parameters: Name Type Description Default directory str Folder containing series/sequence images. Images must be DICOM files. required out_channels int Number of output channels. Only 1 and 3 channels supported. required transforms list Albumentations transformations. See https://albumentations.ai/docs/getting_started/image_augmentation/ . required WW int or list Window width for DICOM images. Single value if using 1 channel or list of 3 values for 3 channels. See https://radiopaedia.org/articles/windowing-ct . required WL int or list Window level for DICOM images. Single value if using 1 channel or list of 3 values for 3 channels. See https://radiopaedia.org/articles/windowing-ct . required Returns: Type Description tensor 4D tensor with dimensions = [channels, number_images/depth, width, height]. See https://pytorch.org/docs/stable/generated/torch.nn.Conv3d.html Examples: >>> i = radtorch . data . VolumeObject ( directory = 'data/PROTOTYPE/DIRECTORY/abdomen/abd_1' ) >>> i . shape torch . Size ([ 1 , 40 , 512 , 512 ])","title":"VolumeObject"},{"location":"data/#radtorch.data.ImageDataset","text":"Creates pytorch dataset(s) and dataloader(s) objects from a parent folder. Use this class for image tasks that invovles handling each single image as a single instance of your dataset. Examples: import radtorch import albumentations as A # Specify image transformations T = A . Compose ([ A . Resize ( 256 , 256 )]) # Create dataset object ds = radtorch . data . ImageDataset ( folder = 'data/4CLASS/' , split = { 'valid' : 0.2 , 'test' : 0.2 }, out_channels = 1 , transforms = { 'train' : T , 'valid' : T , 'test' : T }, ) ds . data_stat () ds . table Parameters: Name Type Description Default folder str Parent folder containing images. radtorch.ImageDataset expects images to be arranged in the following structure: root/ class_1/ image_1 image_2 ... class_2/ image_1 image_2 ... required name str Name to be give to the dataset. If none provided, the current date and time will be used to created a generic dataset name. (default=None) None label_table str|dataframe The table containing data labels for your images. Expected table should contain at least 2 columns: image path column and a label column. Table can be string path to CSV or a pandas dataframe object.(default=None) None instance_id bool True if the data provided in the image path column in label_table contains the image id not the absolute path for the image. (default= False) False add_extension bool If instance_id =True, use this to add extension to image path as needed. Extension must be provided without \".\" e.g. \"dcm\". (default=False) False out_channels int Number of output channels. (default=1) 1 WW int or list Window width for DICOM images. Single value if using 1 channel or list of 3 values for 3 channels. See https://radiopaedia.org/articles/windowing-ct . None WL int or list Window level for DICOM images. Single value if using 1 channel or list of 3 values for 3 channels. See https://radiopaedia.org/articles/windowing-ct . None path_col str Name of the column containing image path data in the label_table. (default='path') 'path' label_col str Name of the column containing label data in the label_table. (default='label') 'label' extension str Type/Extension of images. (default='dcm') 'dcm' transforms dict Dictionary of Albumentations transformations in the form of {'train': .. , 'valid': .. , 'test': .. }. See https://albumentations.ai/docs/getting_started/image_augmentation/ . (default=None) None random_state int Random seed (default=100) 100 sample float Sample or percent of the overall data to be used. (default=1.0) 1.0 split dict dictionary defining how data will be split for training/validation/testing. Follows the sturcture {'valid': float, 'test': float} or {'valid':'float'} in case no testing subset is needed. The percent of the training subset is infered automatically. False ignore_zero_img bool True to ignore images containig all zero pixels. (default=False) False normalize bool True to normalize image data between 0 and 1. (default=True) True batch_size int Dataloader batch size. (default = 16) 16 shuffle bool True to shuffle images during training. (default=True) True weighted_sampler bool True to use a weighted sampler for unbalanced datasets. See https://pytorch.org/docs/stable/data.html#torch.utils.data.WeightedRandomSampler. (default=False) False num_workers int Dataloader CPU workers. (default = 0) 0 Attributes: Name Type Description classes list List of generated classes/labels. class_to_idx dict Dictionary of generated classes/labels and corresponding class/label id. idx_train list List of index values of images/instances used for training subset. These refer to index of ImageDataset.table . idx_valid list List of index values of images/instances used for validation subset. These refer to index of ImageDataset.table . idx_test list List of index values of images/instances used for testing subset. These refer to index of ImageDataset.table . table pandas dataframe Table of images , paths and their labels. table_train pandas dataframe Table of images used for training. Subset of ImageDataset.table . table_valid pandas dataframe Table of images used for validation. Subset of ImageDataset.table . table_test pandas dataframe Table of images used for testing. Subset of ImageDataset.table . tables dict Dictionary of all generated tables in the form of: {'train': table, 'valid':table, 'test':table}. dataset_train pytorch dataset object Training pytorch Dataset dataset_valid pytorch dataset object Validation pytorch Dataset dataset_test pytorch dataset object Testing pytorch Dataset datasets dict Dictionary of all generated Datasets in the form of: {'train': Dataset, 'valid':Dataset, 'test':Dataset}. dataloader_train pytorch dataloader object Training pytorch DataLoader dataloader_valid pytorch dataloader object Validation pytorch DataLoader dataloader_test pytorch dataloader object Testing pytorch DataLoader dataloaders dict Dictionary of all generated Dataloaders in the form of: {'train': Dataloader, 'valid':Dataloader, 'test':Dataloader}. class_weights tensor Values of class weights, for imbalanced datasets, to be used to weight loss functions. See https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss . sampler_weights tensor Values of vlass weights, for imbalanced datasets, to be used to sample from the dataset using Pytroch WeightedRandomSampler. Affects only training dataset not validation or testing. See https://pytorch.org/docs/stable/data.html#torch.utils.data.WeightedRandomSampler","title":"ImageDataset"},{"location":"data/#radtorch.data.ImageDataset-methods","text":"","title":"Methods"},{"location":"data/#radtorch.data.ImageDataset.data_stat","text":"Displays distribution of classes across subsets as table or plot. Parameters: Name Type Description Default plot bool True, display data as figure. False, display data as table. True figsize tuple size of the displayed figure. (8, 6) cmap string Name of Matplotlib color map to be used. See Matplotlib colormaps 'viridis' Returns: Type Description pandas dataframe if plot=False Examples: ds = radtorch . data . ImageDataset ( folder = 'data/4CLASS/' , out_channels = 1 ) ds . data_stat ()","title":"data_stat()"},{"location":"data/#radtorch.data.ImageDataset.view_batch","text":"Displays a batch from a certain subset. Parameters: Name Type Description Default subset string Datasubset to use: either 'train', 'valid', or 'test'. 'train' figsize tuple Size of the displayed figure. (15, 5) cmap string Name of Matplotlib color map to be used. See Matplotlib colormaps 'gray' num_images int Number of displayed image. Usually equals batch_size unless otherwise specified. False rows int Number of rows. 2 Returns: Type Description figure figure containing samples Examples: ds = radtorch . data . ImageDataset ( folder = 'data/4CLASS/' , out_channels = 1 ) ds . view_batch ()","title":"view_batch()"},{"location":"data/#radtorch.data.ImageDataset.view_image","text":"Displays separate images/channels of an image. Parameters: Name Type Description Default id int Target image id in dataset.table (row index). 0 figsize tuple size of the displayed figure. (25, 5) cmap string Name of Matplotlib color map to be used. See Matplotlib colormaps 'gray' Returns: Type Description figure figure containing samples Examples: ds = radtorch . data . ImageDataset ( folder = 'data/4CLASS/' , out_channels = 3 ) ds . view_image ( id = 9 ) ds = radtorch . data . ImageDataset ( folder = 'data/4CLASS/' , out_channels = 3 , WW = [ 1500 , 350 , 80 ], WL = [ - 600 , 50 , 40 ]) ds . view_image ( id = 9 )","title":"view_image()"},{"location":"data/#radtorch.data.VolumeDataset","text":"Dataset object for DICOM Volume. Creates dataset(s) and dataloader(s) ready for training using radtorch or pytorch directly. Parameters: Name Type Description Default folder str Parent folder containing images. radtorch.VolumeDataset expects files to be arranged in the following structure: root/ class_1/ sequence_1/ image_1 image_2 ... sequence_2/ image_1 image_2 ... class_2/ sequence_1/ image_1 image_2 ... sequence_2/ image_1 image_2 ... ... required name str Name to be give to the dataset. If none provided, the current date and time will be used to created a generic dataset name. (default=None) None label_table str|dataframe The table containing data labels for your images. Expected table should contain at least 2 columns: image path column and a label column. Table can be string path to CSV or a pandas dataframe object.(default=None) None use_file bool True to use pre-generated/resampled volume files. To use Volume files: Volume files should be created using radtorch.data.VolumeObject Saved with extension .pt . Placed in the sequence folder. False extension str Type/Extension of images. 'dcm' out_channels int Number of output channels. 1 WW int or list Window width for DICOM images. Single value if using 1 channel or list of 3 values for 3 channels. See https://radiopaedia.org/articles/windowing-ct . None WL int or list Window level for DICOM images. Single value if using 1 channel or list of 3 values for 3 channels. See https://radiopaedia.org/articles/windowing-ct . None path_col str Name of the column containing image path data in the label_table. 'path' label_col str Name of the column containing label data in the label_table. 'label' study_col str Name of the column containing study id in label_table. 'study_id' transforms dict Dictionary of Albumentations transformations in the form of {'train': .. , 'valid': .. , 'test': .. }. See https://albumentations.ai/docs/getting_started/image_augmentation/. NOTE: If using already resampled/created volume files, transformation should be applied during volume creation not dataset i.e. Transforms specified here have no effect during training. None random_state int Random seed. 100 sample float Sample or percent of the overall data to be used. 1.0 split dict dictionary defining how data will be split for training/validation/testing. Follows the sturcture {'valid': float, 'test': float} or {'valid':'float'} in case no testing subset is needed. The percent of the training subset is infered automatically. False normalize bool True to normalize image data between 0 and 1. True batch_size int Dataloader batch size. 16 shuffle bool True to shuffle images during training. True weighted_sampler bool True to use a weighted sampler for unbalanced datasets. See https://pytorch.org/docs/stable/data.html#torch.utils.data.WeightedRandomSampler. False num_workers int Dataloader CPU workers. 0 Attributes: Name Type Description classes list List of generated classes/labels. class_to_idx dict Dictionary of generated classes/labels and corresponding class/label id. idx_train list List of index values of images/instances used for training subset. These refer to index of ImageDataset.table . idx_valid list List of index values of images/instances used for validation subset. These refer to index of ImageDataset.table . idx_test list List of index values of images/instances used for testing subset. These refer to index of ImageDataset.table . table pandas dataframe Table of images , paths and their labels. table_train pandas dataframe Table of images used for training. Subset of ImageDataset.table . table_valid pandas dataframe Table of images used for validation. Subset of ImageDataset.table . table_test pandas dataframe Table of images used for testing. Subset of ImageDataset.table . tables dict Dictionary of all generated tables in the form of: {'train': table, 'valid':table, 'test':table}. dataset_train pytorch dataset object Training pytorch Dataset dataset_valid pytorch dataset object Validation pytorch Dataset dataset_test pytorch dataset object Testing pytorch Dataset datasets dict Dictionary of all generated Datasets in the form of: {'train': Dataset, 'valid':Dataset, 'test':Dataset}. dataloader_train pytorch dataloader object Training pytorch DataLoader dataloader_valid pytorch dataloader object Validation pytorch DataLoader dataloader_test pytorch dataloader object Testing pytorch DataLoader dataloaders dict Dictionary of all generated Dataloaders in the form of: {'train': Dataloader, 'valid':Dataloader, 'test':Dataloader}. class_weights tensor Values of class weights, for imbalanced datasets, to be used to weight loss functions. See https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss . sampler_weights tensor Values of vlass weights, for imbalanced datasets, to be used to sample from the dataset using Pytroch WeightedRandomSampler. Affects only training dataset not validation or testing. See https://pytorch.org/docs/stable/data.html#torch.utils.data.WeightedRandomSampler Examples: import radtorch import albumentations as A # Specify image transformations T = A . Compose ([ A . Resize ( 256 , 256 )]) # Create dataset object ds = radtorch . data . VolumeDataset ( folder = 'data/PROTOTYPE/DIRECTORY/' , split = { 'valid' : 0.3 , 'test' : 0.3 }, out_channels = 1 , transforms = { 'train' : T , 'valid' : T , 'test' : T }, ) ds . data_stat () ds . table","title":"VolumeDataset"},{"location":"data/#radtorch.data.VolumeDataset-methods","text":"","title":"Methods"},{"location":"data/#radtorch.data.VolumeDataset.data_stat","text":"Displays distribution of classes across subsets as table or plot. Parameters: Name Type Description Default plot bool True, display data as figure. False, display data as table. True figsize tuple size of the displayed figure. (8, 6) cmap string Name of Matplotlib color map to be used. See Matplotlib colormaps 'viridis' Returns: Type Description pandas dataframe if plot=False","title":"data_stat()"},{"location":"data/#radtorch.data.VolumeDataset.view_study","text":"Show sample images from a study. Warning This works only with single channel images. Multiple channels are not supported yet here. Parameters: Name Type Description Default id int Target study id in dataset.table (row index). required plane str Anatomical plane to display the images in. Options: 'axial', 'coronal' or 'sagittal'. 'axial' figsize tuple Size of the displayed figure. (15, 15) cols int Number of columns. 5 rows int Number of rows. 5 start int id of the first image to display. 0 end int id of the last image to display. -1 Returns: Type Description figure figure containing images from study. Examples: import radtorch import albumentations as A # Specify image transformations T = A . Compose ([ A . Resize ( 256 , 256 )]) # Create dataset object ds = radtorch . data . VolumeDataset ( folder = 'data/PROTOTYPE/DIRECTORY/' , split = { 'valid' : 0.3 , 'test' : 0.3 }, out_channels = 1 , transforms = { 'train' : T , 'valid' : T , 'test' : T }, ) ds . view_study ( id = 0 , plane = 'axial' ) ds . view_study ( id = 0 , plane = 'coronal' , start = 150 )","title":"view_study()"},{"location":"extractor/","text":"Feature Extractor radtorch.extractor FeatureExtractor Feature Extractor performs feature extraction from images using a pytorch model pretrained on ImageNet . Features can be accessed using below attributes after running the feature extraction process through the .run() method. Parameters: Name Type Description Default model_arch str Model architecture to be used for feature extraction. required dataset ImageDataset ImageDataset to be used for training. required subset str the subset op the dataset to extract features from. Default: 'train'. Options: 'train', 'valid', 'test'. 'train' device str Device to be used for training. Default: 'auto' which automtically detects GPU presence and uses it for feature extraction. Options: 'auto', 'cuda', 'cpu'. 'auto' Attributes: Name Type Description loader pytorch dataloader object Training pytorch DataLoader . table pandas dataframe the table of images to be used for feature extraction. model pytorch neural network Instance of the pytorch model to be used for feature extraction. features pandas dataframe table of extracted features. feature_table pandas dataframe table of extracted features and corresponding uid for each image instance. feature_names list names of the extracted features. Examples: import radtorch import albumentations as A ds = radtorch . data . ImageDataset ( 'data/PROTOTYPE/FILE/' , transforms = { 'train' : A . Compose ([ A . Resize ( 64 , 64 )])}) ext = radtorch . extractor . FeatureExtractor ( 'vgg16' , dataset = ds ) ext . run () Methods hybrid_table ( self , sklearn_ready = False , label_id = True ) Use this method to create pandas dataframes of features and labels that can be used directly into training using scikit-learn. Parameters: Name Type Description Default sklearn_ready bool True returns a tuple of extracted features dataframe and labels dataframe. False returns table of features, uid, path, label and label_id. False label_id bool True returns the label ids (integer) instead of the label string. True num_features ( self ) Returns the expected number of features to be extracted. run ( self ) Runs the feature extraction process","title":"Feature Extractor <small>radtorch.extractor</small>"},{"location":"extractor/#feature-extractor-radtorchextractor","text":"","title":"Feature Extractor radtorch.extractor"},{"location":"extractor/#radtorch.extractor.FeatureExtractor","text":"Feature Extractor performs feature extraction from images using a pytorch model pretrained on ImageNet . Features can be accessed using below attributes after running the feature extraction process through the .run() method. Parameters: Name Type Description Default model_arch str Model architecture to be used for feature extraction. required dataset ImageDataset ImageDataset to be used for training. required subset str the subset op the dataset to extract features from. Default: 'train'. Options: 'train', 'valid', 'test'. 'train' device str Device to be used for training. Default: 'auto' which automtically detects GPU presence and uses it for feature extraction. Options: 'auto', 'cuda', 'cpu'. 'auto' Attributes: Name Type Description loader pytorch dataloader object Training pytorch DataLoader . table pandas dataframe the table of images to be used for feature extraction. model pytorch neural network Instance of the pytorch model to be used for feature extraction. features pandas dataframe table of extracted features. feature_table pandas dataframe table of extracted features and corresponding uid for each image instance. feature_names list names of the extracted features. Examples: import radtorch import albumentations as A ds = radtorch . data . ImageDataset ( 'data/PROTOTYPE/FILE/' , transforms = { 'train' : A . Compose ([ A . Resize ( 64 , 64 )])}) ext = radtorch . extractor . FeatureExtractor ( 'vgg16' , dataset = ds ) ext . run ()","title":"FeatureExtractor"},{"location":"extractor/#radtorch.extractor.FeatureExtractor-methods","text":"","title":"Methods"},{"location":"extractor/#radtorch.extractor.FeatureExtractor.hybrid_table","text":"Use this method to create pandas dataframes of features and labels that can be used directly into training using scikit-learn. Parameters: Name Type Description Default sklearn_ready bool True returns a tuple of extracted features dataframe and labels dataframe. False returns table of features, uid, path, label and label_id. False label_id bool True returns the label ids (integer) instead of the label string. True","title":"hybrid_table()"},{"location":"extractor/#radtorch.extractor.FeatureExtractor.num_features","text":"Returns the expected number of features to be extracted.","title":"num_features()"},{"location":"extractor/#radtorch.extractor.FeatureExtractor.run","text":"Runs the feature extraction process","title":"run()"},{"location":"inference/","text":"Inference radtorch.inference Inference An Inference class creates a predictor object that utilizes a trained model to perform predictions over target image(s). Parameters: Name Type Description Default classifier ImageClassifier trained ImageClassifier . required use_best_model bool True to use the model with the lowest validation loss. True transform list Albumentations transformations. See Image Augmentation . See below. False device str Device to be used for training. Default: 'auto' which automtically detects GPU presence and uses it for feature extraction. Options: 'auto', 'cuda', 'cpu'. 'auto' Using transform By default, the Inference class utilizes the transforms specified in the train subset used to train the ImageClassifier . When this is not available, it will try to utilize transforms of the valid subset. You can specify specific transforms as needed instead. Methods predict ( self , img_path , top_predictions = 'all' , human = True , display_image = False , cmap = 'gray' ) Performs predictions using Inference class Parameters: Name Type Description Default img_path str path to target image. required top_predictions int or str number of top predictions to return. Default = 'all' which returns all predictions. 'all' human bool True to display predictions in human readable format. True display_image bool True to display the target image. False cmap string Name of Matplotlib color map to be used. See Matplotlib colormaps 'gray' Returns: Type Description (list) list of predictions if human is set to False.","title":"Inference <small>radtorch.inference</small>"},{"location":"inference/#inference-radtorchinference","text":"","title":"Inference radtorch.inference"},{"location":"inference/#radtorch.inference.Inference","text":"An Inference class creates a predictor object that utilizes a trained model to perform predictions over target image(s). Parameters: Name Type Description Default classifier ImageClassifier trained ImageClassifier . required use_best_model bool True to use the model with the lowest validation loss. True transform list Albumentations transformations. See Image Augmentation . See below. False device str Device to be used for training. Default: 'auto' which automtically detects GPU presence and uses it for feature extraction. Options: 'auto', 'cuda', 'cpu'. 'auto' Using transform By default, the Inference class utilizes the transforms specified in the train subset used to train the ImageClassifier . When this is not available, it will try to utilize transforms of the valid subset. You can specify specific transforms as needed instead.","title":"Inference"},{"location":"inference/#radtorch.inference.Inference-methods","text":"","title":"Methods"},{"location":"inference/#radtorch.inference.Inference.predict","text":"Performs predictions using Inference class Parameters: Name Type Description Default img_path str path to target image. required top_predictions int or str number of top predictions to return. Default = 'all' which returns all predictions. 'all' human bool True to display predictions in human readable format. True display_image bool True to display the target image. False cmap string Name of Matplotlib color map to be used. See Matplotlib colormaps 'gray' Returns: Type Description (list) list of predictions if human is set to False.","title":"predict()"},{"location":"metrics/","text":"Metrics radtorch.metrics ClassifierMetrics ClassifierMetrics class a set of methods that enables quantiative evaluation of a trained image classifier performance. Parameters: Name Type Description Default classifier ImageClassifier trained ImageClassifier . required use_best_model bool True to use the model with the lowest validation loss. required device str Device to be used for training. Default: 'auto' which automtically detects GPU presence and uses it for feature extraction. Options: 'auto', 'cuda', 'cpu'. 'auto' Methods classification_report ( self , subset = 'test' ) Returns text report showing the main classification metrics. Parameters: Name Type Description Default subset str subset of the dataset to be used. This can be 'train', 'valid', or 'test'. 'test' confusion_matrix ( self , subset = 'test' , figsize = ( 8 , 6 ), cmap = 'Blues' , percent = False ) Returns confusion matrix using target data. Code Adapted from https://github.com/DTrimarchi10/confusion_matrix/blob/master/cf_matrix.py Parameters: Name Type Description Default subset str subset of the dataset to be used. This can be 'train', 'valid', or 'test'. 'test' figsize tuple size of the displayed figure. (8, 6) cmap string Name of Matplotlib color map to be used. See Matplotlib colormaps 'Blues' percent bool True to use percentages instead of real values. False Returns: Type Description figure figure containing confusion matrix get_predictions ( self , subset ) new dataframe is created : self.pred_table, with important columns : label_id and pred_id roc ( self , subset = 'test' , figure_size = ( 8 , 6 ), plot = True ) Displays ROC of the trained classifier and returns ROC-AUC. Parameters: Name Type Description Default subset str subset of the dataset to be used. This can be 'train', 'valid', or 'test'. 'test' figsize tuple size of the displayed figure. required plot bool True to display ROC. True Returns: Type Description float float of ROC-AUC","title":"Metrics <small>radtorch.metrics</small>"},{"location":"metrics/#metrics-radtorchmetrics","text":"","title":"Metrics radtorch.metrics"},{"location":"metrics/#radtorch.metrics.ClassifierMetrics","text":"ClassifierMetrics class a set of methods that enables quantiative evaluation of a trained image classifier performance. Parameters: Name Type Description Default classifier ImageClassifier trained ImageClassifier . required use_best_model bool True to use the model with the lowest validation loss. required device str Device to be used for training. Default: 'auto' which automtically detects GPU presence and uses it for feature extraction. Options: 'auto', 'cuda', 'cpu'. 'auto'","title":"ClassifierMetrics"},{"location":"metrics/#radtorch.metrics.ClassifierMetrics-methods","text":"","title":"Methods"},{"location":"metrics/#radtorch.metrics.ClassifierMetrics.classification_report","text":"Returns text report showing the main classification metrics. Parameters: Name Type Description Default subset str subset of the dataset to be used. This can be 'train', 'valid', or 'test'. 'test'","title":"classification_report()"},{"location":"metrics/#radtorch.metrics.ClassifierMetrics.confusion_matrix","text":"Returns confusion matrix using target data. Code Adapted from https://github.com/DTrimarchi10/confusion_matrix/blob/master/cf_matrix.py Parameters: Name Type Description Default subset str subset of the dataset to be used. This can be 'train', 'valid', or 'test'. 'test' figsize tuple size of the displayed figure. (8, 6) cmap string Name of Matplotlib color map to be used. See Matplotlib colormaps 'Blues' percent bool True to use percentages instead of real values. False Returns: Type Description figure figure containing confusion matrix","title":"confusion_matrix()"},{"location":"metrics/#radtorch.metrics.ClassifierMetrics.get_predictions","text":"new dataframe is created : self.pred_table, with important columns : label_id and pred_id","title":"get_predictions()"},{"location":"metrics/#radtorch.metrics.ClassifierMetrics.roc","text":"Displays ROC of the trained classifier and returns ROC-AUC. Parameters: Name Type Description Default subset str subset of the dataset to be used. This can be 'train', 'valid', or 'test'. 'test' figsize tuple size of the displayed figure. required plot bool True to display ROC. True Returns: Type Description float float of ROC-AUC","title":"roc()"},{"location":"model/","text":"Model radtorch.model Model Model class wraps around pytorch models to return a new modified version of that model. Modifications include changing the number of input channels, output channels or unfreezing the whole neural network. Parameters: Name Type Description Default model_arch str Target model architecture. See pytorch models . required in_channels int Input Channels. required out_classes int Output Classes. required pre_trained bool If True, returns a model pre-trained on ImageNet. required unfreeze_all bool If True, unfreezes all model weights for training, not just the last FC layer. required vgg_avgpool bool When using VGG model, if True: this changes the avg_pool layer to use size (1,1) instead of the default value. Set this to True if the vgg_fc is True. required vgg_fc bool When using VGG model, if True: the classifier part of the model will be replaced by a single nn.Linear layer. required Returns: Type Description model Instance of the target pytorch model with desired modifications. Models with 1 input channels Since most of pre-trained models were trained using ImageNet 3-channel images (i.e. RGB), changing the number of input channels to 1 will replace the first Conv2d layer in the model with a new layer. The new layer weights will be initiallized as noted here . The new layer wweights will be unfrozen by default during training unless you specify so. Output FC layer The output FC ( nn.Linear ) layer will be unfrozen by default for training. Using VGG models When using VGG models, vgg_fc parameter should be specified. Usually set to its default value True . Examples: >>> m = radtorch . model . Model ( 'vgg16' , 3 , 10 ) >>> m VGG ( ( features ): Sequential ( ( 0 ): Conv2d ( 3 , 64 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) #<<< Number of input channels is 3 ( 1 ): ReLU ( inplace = True ) ( 2 ): Conv2d ( 64 , 64 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) ( 3 ): ReLU ( inplace = True ) ( 4 ): MaxPool2d ( kernel_size = 2 , stride = 2 , padding = 0 , dilation = 1 , ceil_mode = False ) ( 5 ): Conv2d ( 64 , 128 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) ( 6 ): ReLU ( inplace = True ) ( 7 ): Conv2d ( 128 , 128 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) ( 8 ): ReLU ( inplace = True ) ( 9 ): MaxPool2d ( kernel_size = 2 , stride = 2 , padding = 0 , dilation = 1 , ceil_mode = False ) ( 10 ): Conv2d ( 128 , 256 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) ( 11 ): ReLU ( inplace = True ) ( 12 ): Conv2d ( 256 , 256 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) ( 13 ): ReLU ( inplace = True ) ( 14 ): Conv2d ( 256 , 256 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) ( 15 ): ReLU ( inplace = True ) ( 16 ): MaxPool2d ( kernel_size = 2 , stride = 2 , padding = 0 , dilation = 1 , ceil_mode = False ) ( 17 ): Conv2d ( 256 , 512 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) ( 18 ): ReLU ( inplace = True ) ( 19 ): Conv2d ( 512 , 512 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) ( 20 ): ReLU ( inplace = True ) ( 21 ): Conv2d ( 512 , 512 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) ( 22 ): ReLU ( inplace = True ) ( 23 ): MaxPool2d ( kernel_size = 2 , stride = 2 , padding = 0 , dilation = 1 , ceil_mode = False ) ( 24 ): Conv2d ( 512 , 512 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) ( 25 ): ReLU ( inplace = True ) ( 26 ): Conv2d ( 512 , 512 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) ( 27 ): ReLU ( inplace = True ) ( 28 ): Conv2d ( 512 , 512 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) ( 29 ): ReLU ( inplace = True ) ( 30 ): MaxPool2d ( kernel_size = 2 , stride = 2 , padding = 0 , dilation = 1 , ceil_mode = False ) ) ( avgpool ): AdaptiveAvgPool2d ( output_size = ( 1 , 1 )) ( classifier ): Linear ( in_features = 512 , out_features = 10 , bias = True ) #<<< Classifier is one layer with output = 10 ) >>> m = radtorch . model . Model ( 'vgg16' , 1 , 10 ) >>> m VGG ( ( features ): Sequential ( ( 0 ): Conv2d ( 1 , 64 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) #<<< Number of input channels is 1 ( 1 ): ReLU ( inplace = True ) ( 2 ): Conv2d ( 64 , 64 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) ( 3 ): ReLU ( inplace = True ) ( 4 ): MaxPool2d ( kernel_size = 2 , stride = 2 , padding = 0 , dilation = 1 , ceil_mode = False ) ( 5 ): Conv2d ( 64 , 128 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) ( 6 ): ReLU ( inplace = True ) ( 7 ): Conv2d ( 128 , 128 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) ( 8 ): ReLU ( inplace = True ) ( 9 ): MaxPool2d ( kernel_size = 2 , stride = 2 , padding = 0 , dilation = 1 , ceil_mode = False ) ( 10 ): Conv2d ( 128 , 256 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) ( 11 ): ReLU ( inplace = True ) ( 12 ): Conv2d ( 256 , 256 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) ( 13 ): ReLU ( inplace = True ) ( 14 ): Conv2d ( 256 , 256 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) ( 15 ): ReLU ( inplace = True ) ( 16 ): MaxPool2d ( kernel_size = 2 , stride = 2 , padding = 0 , dilation = 1 , ceil_mode = False ) ( 17 ): Conv2d ( 256 , 512 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) ( 18 ): ReLU ( inplace = True ) ( 19 ): Conv2d ( 512 , 512 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) ( 20 ): ReLU ( inplace = True ) ( 21 ): Conv2d ( 512 , 512 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) ( 22 ): ReLU ( inplace = True ) ( 23 ): MaxPool2d ( kernel_size = 2 , stride = 2 , padding = 0 , dilation = 1 , ceil_mode = False ) ( 24 ): Conv2d ( 512 , 512 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) ( 25 ): ReLU ( inplace = True ) ( 26 ): Conv2d ( 512 , 512 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) ( 27 ): ReLU ( inplace = True ) ( 28 ): Conv2d ( 512 , 512 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) ( 29 ): ReLU ( inplace = True ) ( 30 ): MaxPool2d ( kernel_size = 2 , stride = 2 , padding = 0 , dilation = 1 , ceil_mode = False ) ) ( avgpool ): AdaptiveAvgPool2d ( output_size = ( 1 , 1 )) ( classifier ): Linear ( in_features = 512 , out_features = 10 , bias = True ) #<<< Classifier is one layer with output = 10 ) >>> m = radtorch . model . Model ( 'vgg16' , 1 , 10 , vgg_avgpool = False , vgg_fc = False ) >>> m VGG ( ( features ): Sequential ( ( 0 ): Conv2d ( 1 , 64 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) #<<< Number of input channels is 3 ( 1 ): ReLU ( inplace = True ) ( 2 ): Conv2d ( 64 , 64 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) ( 3 ): ReLU ( inplace = True ) ( 4 ): MaxPool2d ( kernel_size = 2 , stride = 2 , padding = 0 , dilation = 1 , ceil_mode = False ) ( 5 ): Conv2d ( 64 , 128 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) ( 6 ): ReLU ( inplace = True ) ( 7 ): Conv2d ( 128 , 128 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) ( 8 ): ReLU ( inplace = True ) ( 9 ): MaxPool2d ( kernel_size = 2 , stride = 2 , padding = 0 , dilation = 1 , ceil_mode = False ) ( 10 ): Conv2d ( 128 , 256 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) ( 11 ): ReLU ( inplace = True ) ( 12 ): Conv2d ( 256 , 256 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) ( 13 ): ReLU ( inplace = True ) ( 14 ): Conv2d ( 256 , 256 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) ( 15 ): ReLU ( inplace = True ) ( 16 ): MaxPool2d ( kernel_size = 2 , stride = 2 , padding = 0 , dilation = 1 , ceil_mode = False ) ( 17 ): Conv2d ( 256 , 512 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) ( 18 ): ReLU ( inplace = True ) ( 19 ): Conv2d ( 512 , 512 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) ( 20 ): ReLU ( inplace = True ) ( 21 ): Conv2d ( 512 , 512 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) ( 22 ): ReLU ( inplace = True ) ( 23 ): MaxPool2d ( kernel_size = 2 , stride = 2 , padding = 0 , dilation = 1 , ceil_mode = False ) ( 24 ): Conv2d ( 512 , 512 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) ( 25 ): ReLU ( inplace = True ) ( 26 ): Conv2d ( 512 , 512 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) ( 27 ): ReLU ( inplace = True ) ( 28 ): Conv2d ( 512 , 512 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) ( 29 ): ReLU ( inplace = True ) ( 30 ): MaxPool2d ( kernel_size = 2 , stride = 2 , padding = 0 , dilation = 1 , ceil_mode = False ) ) ( avgpool ): AdaptiveAvgPool2d ( output_size = ( 7 , 7 )) ( classifier ): Sequential ( ( 0 ): Linear ( in_features = 25088 , out_features = 4096 , bias = True ) ( 1 ): ReLU ( inplace = True ) ( 2 ): Dropout ( p = 0.5 , inplace = False ) ( 3 ): Linear ( in_features = 4096 , out_features = 4096 , bias = True ) ( 4 ): ReLU ( inplace = True ) ( 5 ): Dropout ( p = 0.5 , inplace = False ) ( 6 ): Linear ( in_features = 4096 , out_features = 10 , bias = True ) #<<< Output classes = 10 ) )","title":"Model <small>radtorch.model</small>"},{"location":"model/#model-radtorchmodel","text":"","title":"Model radtorch.model"},{"location":"model/#radtorch.model.Model","text":"Model class wraps around pytorch models to return a new modified version of that model. Modifications include changing the number of input channels, output channels or unfreezing the whole neural network. Parameters: Name Type Description Default model_arch str Target model architecture. See pytorch models . required in_channels int Input Channels. required out_classes int Output Classes. required pre_trained bool If True, returns a model pre-trained on ImageNet. required unfreeze_all bool If True, unfreezes all model weights for training, not just the last FC layer. required vgg_avgpool bool When using VGG model, if True: this changes the avg_pool layer to use size (1,1) instead of the default value. Set this to True if the vgg_fc is True. required vgg_fc bool When using VGG model, if True: the classifier part of the model will be replaced by a single nn.Linear layer. required Returns: Type Description model Instance of the target pytorch model with desired modifications. Models with 1 input channels Since most of pre-trained models were trained using ImageNet 3-channel images (i.e. RGB), changing the number of input channels to 1 will replace the first Conv2d layer in the model with a new layer. The new layer weights will be initiallized as noted here . The new layer wweights will be unfrozen by default during training unless you specify so. Output FC layer The output FC ( nn.Linear ) layer will be unfrozen by default for training. Using VGG models When using VGG models, vgg_fc parameter should be specified. Usually set to its default value True . Examples: >>> m = radtorch . model . Model ( 'vgg16' , 3 , 10 ) >>> m VGG ( ( features ): Sequential ( ( 0 ): Conv2d ( 3 , 64 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) #<<< Number of input channels is 3 ( 1 ): ReLU ( inplace = True ) ( 2 ): Conv2d ( 64 , 64 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) ( 3 ): ReLU ( inplace = True ) ( 4 ): MaxPool2d ( kernel_size = 2 , stride = 2 , padding = 0 , dilation = 1 , ceil_mode = False ) ( 5 ): Conv2d ( 64 , 128 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) ( 6 ): ReLU ( inplace = True ) ( 7 ): Conv2d ( 128 , 128 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) ( 8 ): ReLU ( inplace = True ) ( 9 ): MaxPool2d ( kernel_size = 2 , stride = 2 , padding = 0 , dilation = 1 , ceil_mode = False ) ( 10 ): Conv2d ( 128 , 256 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) ( 11 ): ReLU ( inplace = True ) ( 12 ): Conv2d ( 256 , 256 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) ( 13 ): ReLU ( inplace = True ) ( 14 ): Conv2d ( 256 , 256 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) ( 15 ): ReLU ( inplace = True ) ( 16 ): MaxPool2d ( kernel_size = 2 , stride = 2 , padding = 0 , dilation = 1 , ceil_mode = False ) ( 17 ): Conv2d ( 256 , 512 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) ( 18 ): ReLU ( inplace = True ) ( 19 ): Conv2d ( 512 , 512 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) ( 20 ): ReLU ( inplace = True ) ( 21 ): Conv2d ( 512 , 512 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) ( 22 ): ReLU ( inplace = True ) ( 23 ): MaxPool2d ( kernel_size = 2 , stride = 2 , padding = 0 , dilation = 1 , ceil_mode = False ) ( 24 ): Conv2d ( 512 , 512 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) ( 25 ): ReLU ( inplace = True ) ( 26 ): Conv2d ( 512 , 512 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) ( 27 ): ReLU ( inplace = True ) ( 28 ): Conv2d ( 512 , 512 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) ( 29 ): ReLU ( inplace = True ) ( 30 ): MaxPool2d ( kernel_size = 2 , stride = 2 , padding = 0 , dilation = 1 , ceil_mode = False ) ) ( avgpool ): AdaptiveAvgPool2d ( output_size = ( 1 , 1 )) ( classifier ): Linear ( in_features = 512 , out_features = 10 , bias = True ) #<<< Classifier is one layer with output = 10 ) >>> m = radtorch . model . Model ( 'vgg16' , 1 , 10 ) >>> m VGG ( ( features ): Sequential ( ( 0 ): Conv2d ( 1 , 64 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) #<<< Number of input channels is 1 ( 1 ): ReLU ( inplace = True ) ( 2 ): Conv2d ( 64 , 64 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) ( 3 ): ReLU ( inplace = True ) ( 4 ): MaxPool2d ( kernel_size = 2 , stride = 2 , padding = 0 , dilation = 1 , ceil_mode = False ) ( 5 ): Conv2d ( 64 , 128 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) ( 6 ): ReLU ( inplace = True ) ( 7 ): Conv2d ( 128 , 128 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) ( 8 ): ReLU ( inplace = True ) ( 9 ): MaxPool2d ( kernel_size = 2 , stride = 2 , padding = 0 , dilation = 1 , ceil_mode = False ) ( 10 ): Conv2d ( 128 , 256 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) ( 11 ): ReLU ( inplace = True ) ( 12 ): Conv2d ( 256 , 256 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) ( 13 ): ReLU ( inplace = True ) ( 14 ): Conv2d ( 256 , 256 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) ( 15 ): ReLU ( inplace = True ) ( 16 ): MaxPool2d ( kernel_size = 2 , stride = 2 , padding = 0 , dilation = 1 , ceil_mode = False ) ( 17 ): Conv2d ( 256 , 512 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) ( 18 ): ReLU ( inplace = True ) ( 19 ): Conv2d ( 512 , 512 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) ( 20 ): ReLU ( inplace = True ) ( 21 ): Conv2d ( 512 , 512 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) ( 22 ): ReLU ( inplace = True ) ( 23 ): MaxPool2d ( kernel_size = 2 , stride = 2 , padding = 0 , dilation = 1 , ceil_mode = False ) ( 24 ): Conv2d ( 512 , 512 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) ( 25 ): ReLU ( inplace = True ) ( 26 ): Conv2d ( 512 , 512 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) ( 27 ): ReLU ( inplace = True ) ( 28 ): Conv2d ( 512 , 512 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) ( 29 ): ReLU ( inplace = True ) ( 30 ): MaxPool2d ( kernel_size = 2 , stride = 2 , padding = 0 , dilation = 1 , ceil_mode = False ) ) ( avgpool ): AdaptiveAvgPool2d ( output_size = ( 1 , 1 )) ( classifier ): Linear ( in_features = 512 , out_features = 10 , bias = True ) #<<< Classifier is one layer with output = 10 ) >>> m = radtorch . model . Model ( 'vgg16' , 1 , 10 , vgg_avgpool = False , vgg_fc = False ) >>> m VGG ( ( features ): Sequential ( ( 0 ): Conv2d ( 1 , 64 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) #<<< Number of input channels is 3 ( 1 ): ReLU ( inplace = True ) ( 2 ): Conv2d ( 64 , 64 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) ( 3 ): ReLU ( inplace = True ) ( 4 ): MaxPool2d ( kernel_size = 2 , stride = 2 , padding = 0 , dilation = 1 , ceil_mode = False ) ( 5 ): Conv2d ( 64 , 128 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) ( 6 ): ReLU ( inplace = True ) ( 7 ): Conv2d ( 128 , 128 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) ( 8 ): ReLU ( inplace = True ) ( 9 ): MaxPool2d ( kernel_size = 2 , stride = 2 , padding = 0 , dilation = 1 , ceil_mode = False ) ( 10 ): Conv2d ( 128 , 256 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) ( 11 ): ReLU ( inplace = True ) ( 12 ): Conv2d ( 256 , 256 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) ( 13 ): ReLU ( inplace = True ) ( 14 ): Conv2d ( 256 , 256 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) ( 15 ): ReLU ( inplace = True ) ( 16 ): MaxPool2d ( kernel_size = 2 , stride = 2 , padding = 0 , dilation = 1 , ceil_mode = False ) ( 17 ): Conv2d ( 256 , 512 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) ( 18 ): ReLU ( inplace = True ) ( 19 ): Conv2d ( 512 , 512 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) ( 20 ): ReLU ( inplace = True ) ( 21 ): Conv2d ( 512 , 512 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) ( 22 ): ReLU ( inplace = True ) ( 23 ): MaxPool2d ( kernel_size = 2 , stride = 2 , padding = 0 , dilation = 1 , ceil_mode = False ) ( 24 ): Conv2d ( 512 , 512 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) ( 25 ): ReLU ( inplace = True ) ( 26 ): Conv2d ( 512 , 512 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) ( 27 ): ReLU ( inplace = True ) ( 28 ): Conv2d ( 512 , 512 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = ( 1 , 1 )) ( 29 ): ReLU ( inplace = True ) ( 30 ): MaxPool2d ( kernel_size = 2 , stride = 2 , padding = 0 , dilation = 1 , ceil_mode = False ) ) ( avgpool ): AdaptiveAvgPool2d ( output_size = ( 7 , 7 )) ( classifier ): Sequential ( ( 0 ): Linear ( in_features = 25088 , out_features = 4096 , bias = True ) ( 1 ): ReLU ( inplace = True ) ( 2 ): Dropout ( p = 0.5 , inplace = False ) ( 3 ): Linear ( in_features = 4096 , out_features = 4096 , bias = True ) ( 4 ): ReLU ( inplace = True ) ( 5 ): Dropout ( p = 0.5 , inplace = False ) ( 6 ): Linear ( in_features = 4096 , out_features = 10 , bias = True ) #<<< Output classes = 10 ) )","title":"Model"},{"location":"start/","text":"Getting Started Install Stable version To install stable version via pip, use the following command: pip3 install git+https://download.radtorch.com/ Preview version To install Preview beta version via pip, use the following command: pip3 install git+https://beta.radtorch.com/ Installing preview version Preview version is continuously updated and is not final. Components might not be working. Prerequisites It is recommended that you use Python 3.7 or greater. The following libraries are required and automatically installed by radtorch: numpy, matplotlib, pandas, pydicom, SimpleITK, torchinfo, shap, grad_cam, torch, torchvision, seaborn, albumentations, pillow, sklearn, tqdm, pathlib, seaborn-image.","title":"Getting Started"},{"location":"start/#getting-started","text":"","title":"Getting Started"},{"location":"start/#install","text":"","title":"Install"},{"location":"start/#stable-version","text":"To install stable version via pip, use the following command: pip3 install git+https://download.radtorch.com/","title":"Stable version"},{"location":"start/#preview-version","text":"To install Preview beta version via pip, use the following command: pip3 install git+https://beta.radtorch.com/ Installing preview version Preview version is continuously updated and is not final. Components might not be working.","title":"Preview version"},{"location":"start/#prerequisites","text":"It is recommended that you use Python 3.7 or greater. The following libraries are required and automatically installed by radtorch: numpy, matplotlib, pandas, pydicom, SimpleITK, torchinfo, shap, grad_cam, torch, torchvision, seaborn, albumentations, pillow, sklearn, tqdm, pathlib, seaborn-image.","title":"Prerequisites"},{"location":"support/","text":"Have a Question? Feel free to ask questions or submit issues on the GitHub here: https://github.com/radtorch/radtorch/issues","title":"Help"},{"location":"support/#have-a-question","text":"Feel free to ask questions or submit issues on the GitHub here: https://github.com/radtorch/radtorch/issues","title":"Have a Question?"},{"location":"version/","text":"Version Log Stable Release 1.0.0 4/15/2022 data: ImageObject, ImageDataset, VolumeObject, VolumeDataset. model: Model, ModelBase. classifier: ImageClassifier. extractor: FeatureExtractor. metrics: ClassifierMetrics. inference: Inference xai: XAI. Preview Release 0.8.1 8/4/2020 0.6 6/20/2020 0.5 5/14/2020 0.4 4/30/2020 0.3 3/29/2020 0.2 3/17/2020 0.1.1b 3/3/2020","title":"Version Log"},{"location":"version/#version-log","text":"","title":"Version Log"},{"location":"version/#stable-release","text":"","title":"Stable Release"},{"location":"version/#100","text":"4/15/2022 data: ImageObject, ImageDataset, VolumeObject, VolumeDataset. model: Model, ModelBase. classifier: ImageClassifier. extractor: FeatureExtractor. metrics: ClassifierMetrics. inference: Inference xai: XAI.","title":"1.0.0"},{"location":"version/#preview-release","text":"","title":"Preview Release"},{"location":"version/#081","text":"8/4/2020","title":"0.8.1"},{"location":"version/#06","text":"6/20/2020","title":"0.6"},{"location":"version/#05","text":"5/14/2020","title":"0.5"},{"location":"version/#04","text":"4/30/2020","title":"0.4"},{"location":"version/#03","text":"3/29/2020","title":"0.3"},{"location":"version/#02","text":"3/17/2020","title":"0.2"},{"location":"version/#011b","text":"3/3/2020","title":"0.1.1b"}]}